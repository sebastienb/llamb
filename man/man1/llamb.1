.TH LLAMB 1 "May 2024" "llamb 2.1.1" "User Commands"
.SH NAME
llamb \- CLI LLM client that answers questions directly from your terminal
.SH SYNOPSIS
.B llamb
[\fIOPTIONS\fR]
[\fIQUESTION\fR...]
.SH DESCRIPTION
.B llamb
is a command-line interface for working with Large Language Models (LLMs) directly from your terminal.
It supports multiple providers (OpenAI, Anthropic, Mistral, etc.), conversation history management,
file input/output, terminal-specific sessions, smart error handling for offline providers,
saved prompt templates, automatic file extension detection for code outputs,
and comprehensive provider management.
.SH OPTIONS
.TP
.B \-m, \-\-model <model>
Specify the model to use (defaults to provider's default model)
.TP
.B \-p, \-\-provider <provider>
Specify the provider to use (defaults to the configured default provider)
.TP
.B \-u, \-\-baseUrl <baseUrl>
Specify a custom base URL for this request
.TP
.B \-s, \-\-stream
Stream the response as it arrives (default: true)
.TP
.B \-\-progress\-only
Show progress indicator without streaming content (prevents scrollback artifacts)
.TP
.B \-\-live\-stream
Force live streaming of content even if progress-only mode is enabled
.TP
.B \-\-ink
Use ink-based UI for rendering (prevents scrollback artifacts, default: enabled)
.TP
.B \-\-no\-ink
Disable ink-based UI rendering and use traditional rendering
.TP
.B \-c, \-\-chat
Enable continuous conversation mode for follow-up questions
.TP
.B \-n, \-\-no\-history
Do not use conversation history for this request
.TP
.B \-f, \-\-file <path>
Path to a file to include with your question
.TP
.B \-j, \-\-jina <url>
URL to fetch content from using Jina Reader
.TP
.B \-o, \-\-output [path]
Save the response to a file. If path is not provided, you will be prompted for a filename. When saving code responses, appropriate file extensions will be automatically applied if the response is a pure code block.
.TP
.B \-t, \-\-prompt <name>
Use a saved prompt template for your question
.TP
.B \-\-overwrite
Overwrite existing files without prompting
.TP
.B \-h, \-\-help
Display help information
.TP
.B \-V, \-\-version
Display version information
.SH COMMANDS
.TP
.B llamb [question...]
Ask a question to the LLM
.TP
.B llamb providers
List configured LLM providers
.TP
.B llamb provider:add
Add a new provider configuration
.TP
.B llamb provider:edit
Edit an existing provider configuration
.TP
.B llamb provider:delete
Delete a provider configuration
.TP
.B llamb provider:apikey
Update API key for a provider
.TP
.B llamb provider:default
Set the default provider
.TP
.B llamb models
List available models for a provider
.TP
.B llamb model:default
Set the default model for a provider
.TP
.B llamb context:clear
Clear the current conversation context
.TP
.B llamb context:new
Start a new conversation context
.TP
.B llamb context:history
Display the current conversation history
.TP
.B llamb context:debug
Display terminal and session debugging information
.TP
.B llamb config:progress-mode
Configure UI rendering mode (ink, progress-only, or traditional)
.TP
.B llamb prompt:list
List all saved prompt templates
.TP
.B llamb prompt:add
Create a new prompt template
.TP
.B llamb prompt:edit
Edit an existing prompt template
.TP
.B llamb prompt:delete
Delete a prompt template
.TP
.B llamb prompt:show
Show details of a specific prompt template
.TP
.B llamb jina:apikey
Set or update the Jina Reader API key
.TP
.B llamb jina:test <url>
Test the Jina Reader API with a URL
.SH SLASH COMMANDS
Llamb supports short-form slash commands when entering questions. In continuous conversation mode (-c), there are additional slash commands available:
.TP
.B /models
Equivalent to "llamb models"
.TP
.B /model
Equivalent to "llamb model:default"
.TP
.B /providers
Equivalent to "llamb providers"
.TP
.B /clear
Equivalent to "llamb context:clear"
.TP
.B /new
Equivalent to "llamb context:new"
.TP
.B /history
Equivalent to "llamb context:history"
.TP
.B /debug
Equivalent to "llamb context:debug"
.TP
.B /exit, /quit
Exit continuous conversation mode (available only in chat mode)
.TP
.B /file
Attach a file to your next question (available only in chat mode)
.TP
.B /unfile
Remove the attached file (available only in chat mode)
.TP
.B /help
Show available commands in continuous conversation mode
.SH EXAMPLES
.TP
Ask a simple question:
.B llamb "What is the capital of France?"
.TP
Include a file with your question:
.B llamb -f script.js "Explain this code"
.TP
Process file contents:
.B llamb "Summarize this" -f document.txt
.TP
Fetch URL content using Jina Reader:
.B llamb -j https://example.com "Explain this website"
.TP
Set or update the Jina Reader API key:
.B llamb jina:apikey
.TP
Test Jina Reader with a URL:
.B llamb jina:test https://example.com
.TP
Save response (prompts for filename):
.B llamb "Generate JSON" -o
.TP
Save response to a specific file:
.B llamb "Generate JSON" -o result.json
.TP
Start in continuous conversation mode for follow-up questions:
.B llamb -c "Tell me about TypeScript"
.TP
Ask without using conversation history:
.B llamb -n "What is 2+2?"
.TP
View conversation history:
.B llamb /history
.TP
Clear conversation history:
.B llamb /clear
.TP
Start a new conversation:
.B llamb /new
.TP
Show terminal session debug info:
.B llamb /debug
.TP
Change the default model for current provider:
.B llamb /model
.TP
Change the default model for a specific provider:
.B llamb model:default -p openai
.TP
Edit an existing provider:
.B llamb provider:edit
.TP
Edit a provider non-interactively:
.B llamb provider:edit --name openai --url https://api.openai.com/v1 --model gpt-4o
.TP
Delete a provider interactively:
.B llamb provider:delete
.TP
Delete a provider non-interactively:
.B llamb provider:delete --name openai
.TP
Delete a provider without confirmation:
.B llamb provider:delete --name openai --force
.TP
Use ink-based UI (default):
.B llamb "What is the capital of France?"
.TP
Disable ink-based UI and use traditional rendering:
.B llamb --no-ink "What is the capital of France?"
.TP
Use progress-only mode to prevent scrollback artifacts:
.B llamb --progress-only "What is the capital of France?"
.TP
Configure UI rendering mode:
.B llamb config:progress-mode --ink
.TP
Use a saved prompt template:
.B llamb -t summarize -f document.txt
.TP
Use a prompt template with direct input:
.B llamb -t code-review "function add(a, b) { return a + b; }"
.TP
Save a code response with automatic file type detection:
.B llamb "Write a Python function to calculate fibonacci" -o fibonacci
.SH PROMPT TEMPLATES
Prompt templates allow you to save and reuse common prompts with placeholders:
.TP
.B {input}
Gets replaced with the command-line question text
.TP
.B {file}
Gets replaced with file contents when using -f flag
.TP
.B {filename}
Gets replaced with the filename when using -f flag
.PP
For example, a prompt template could be:
.PP
.nf
Please analyze the following code from {filename}:

{file}

Focus on these aspects:
1. Performance
2. Security
3. Readability

Additional notes: {input}
.fi
.PP
This can be used with:
.B llamb -t analyze -f script.js "Look for memory leaks"
.SH SMART FILE EXTENSION DETECTION
When saving responses to files without specifying an extension, LLaMB automatically:
.TP
- Applies language-specific extensions for pure code block responses (.js, .py, etc.)
.TP
- Defaults to .txt for mixed content (explanations + code)
.TP
- Always respects explicitly provided file extensions
.SH ENVIRONMENT
.TP
.B OPENAI_API_KEY
If set, will be used as the default API key for OpenAI
.SH FILES
.TP
.B ~/.llamb/sessions/
Directory where conversation sessions are stored
.TP
.B ~/.config/llamb/
Configuration directory for provider settings and prompt templates
.SH NOTES
Terminal-specific sessions are determined using environment variables such as TTY, PID, and terminal-specific session IDs.
Each terminal window will maintain its own conversation history.
.SH AUTHOR
This manual page was written for the llamb project.
.SH SEE ALSO
Full documentation is available at: https://github.com/yourgithub/llamb